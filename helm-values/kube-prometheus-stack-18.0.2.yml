# helm upgrade --install prometheus prometheus-community/kube-prometheus-stack -n kube-prometheus-stack --create-namespace --values helm-values/kube-prometheus-stack-18.0.2.yml --version 18.0.2


  ## Provide a name in place of kube-prometheus-stack for `app:` labels
##
nameOverride: ""

## Override the deployment namespace
##
namespaceOverride: "kube-prometheus-stack"

## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
##
kubeTargetVersionOverride: ""

## Allow kubeVersion to be overridden while creating the ingress
##
kubeVersionOverride: ""

## Provide a name to substitute for the full names of resources
##
fullnameOverride: ""

## Labels to apply to all resources
##
commonLabels: {}
# scmhash: abc123
# myLabel: aakkmd

## Create default rules for monitoring the cluster
##
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    general: true
    k8s: true
    kubeApiserver: true
    kubeApiserverAvailability: true
    kubeApiserverError: true
    kubeApiserverSlos: true
    kubelet: true
    kubePrometheusGeneral: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubernetesAbsent: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeScheduler: true
    kubeStateMetrics: true
    network: true
    node: true
    prometheus: true
    prometheusOperator: true
    time: true

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:
  ## Deploy alertmanager
  ##
  enabled: true

  ## Settings affecting alertmanagerSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec
  ##
  alertmanagerSpec:

    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
    ##
    retention: 120h

    ## Storage is the definition of how storage will be used by the Alertmanager instances.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
    storage: {}
    # volumeClaimTemplate:
    #   spec:
    #     storageClassName: gluster
    #     accessModes: ["ReadWriteOnce"]
    #     resources:
    #       requests:
    #         storage: 50Gi
    #   selector: {}


    ## Define resources requests and limits for single Pods.
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
     requests:
       memory: 400Mi


## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
##
grafana:
  enabled: true
  namespaceOverride: ""
  persistence:
    enabled: true

  env:
    GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: grafana-groupedbarchart-panel
#    GF_SERVER_DOMAIN: grafana.highcohesion.com
#    GF_SERVER_HTTP_PORT: 80
#  To download custom plugins
#  https://stackoverflow.com/questions/58752486/how-to-install-custom-plugin-for-grafana-running-in-kubernetes-cluster-on-azure
  extraInitContainers:
    - name: local-plugins-downloader
      image: busybox
      command:
        - /bin/sh
        - -c
        - |
          #!/bin/sh
          set -euo pipefail
          mkdir -p /var/lib/grafana/plugins
          cd /var/lib/grafana/plugins
          for url in https://github.com/gipong/grafana-groupedbarchart-panel/archive/refs/heads/master.zip; do
            wget --no-check-certificate $url -O temp.zip
            unzip temp.zip || echo true-zip
            rm temp.zip || echo true-delete
          done
      volumeMounts:
        - name: storage
          mountPath: /var/lib/grafana

  plugins: []
#   - digrich-bubblechart-panel
#   - grafana-clock-panel

  ## Deploy default dashboards.
  ##
  defaultDashboardsEnabled: true

  defaultDashboardsTimezone: browser

  adminPassword: mQ4ePQScHvjmEo1Q01bI14uJ96

  ingress:
    ## If true, Grafana Ingress will be created
    ##
    enabled: true

    ## Annotations for Grafana Ingress
    ##
    annotations:
       kubernetes.io/ingress.class: kong
       konghq.com/protocols: "https"
       konghq.com/https-redirect-status-code: "301"
       konghq.com/preserve-host: "true"

    ## Hostnames.
    ## Must be provided if Ingress is enable.
    ##
    hosts:
      - grafana-develop.highcohesion.com

    ## Path for grafana ingress
    path: /

    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    ##
    tls:
     - secretName: grafana-general-tls
       hosts:
       - grafana-develop.highcohesion.com


  ## Passed to grafana subchart and used by servicemonitor below
  ##
  service:
    portName: service


## Component scraping the kube api server
##
kubeApiServer:
  enabled: true

## Component scraping the kubelet and kubelet-hosted cAdvisor
##
kubelet:
  enabled: true
  namespace: kube-system


## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: true

  serviceMonitor:
    enabled: true

## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
  enabled: true

## Component scraping kubeDns. Use either this or coreDns
##
kubeDns:
  enabled: false

## Component scraping etcd
##
kubeEtcd:
  enabled: true

  serviceMonitor:
    enabled: true


## Component scraping kube scheduler
##
kubeScheduler:
  enabled: true

  serviceMonitor:
    enabled: true

## Component scraping kube proxy
##
kubeProxy:
  enabled: true

  serviceMonitor:
    enabled: true


## Component scraping kube state metrics
##
kubeStateMetrics:
  enabled: true

## Configuration for kube-state-metrics subchart
##
kube-state-metrics:
  namespaceOverride: ""
  rbac:
    create: true
  podSecurityPolicy:
    enabled: true

## Deploy node exporter as a daemonset to all nodes
##
nodeExporter:
  enabled: true



## Manages Prometheus and Alertmanager components
##
prometheusOperator:
  enabled: true

  ## Resource limits & requests
  ##
  resources:
   limits:
     cpu: 200m
     memory: 200Mi
   requests:
     cpu: 100m
     memory: 100Mi

  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
  ##
  hostNetwork: false


## Deploy a Prometheus instance
##
prometheus:
  enabled: true

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  ##
#  prometheusSpec:
#    additionalScrapeConfigs:
#      - job_name: 'functions'
#        # Override the global default and scrape targets from this job every 5 seconds.
#        scrape_interval: 5s
#        static_configs:
#          # Specify all the Fn servers from which metrics will be scraped
#          - targets: [ 'function-fn-fn.fn:80' ] # Uses /metrics by default
#
#      - job_name: 'functions-lb'
#        # Override the global default and scrape targets from this job every 5 seconds.
#        scrape_interval: 5s
#        static_configs:
#          # Specify all the Fn servers from which metrics will be scraped
#          - targets: [ 'function-fn-fn.fn:90' ] # Uses /metrics by default
#
#      - job_name: 'functions-flow'
#        # Override the global default and scrape targets from this job every 5 seconds.
#        scrape_interval: 5s
#        static_configs:
#          # Specify all the Fn servers from which metrics will be scraped
#          - targets: [ 'function-fn-fn-flow.fn:80' ] # Uses /metrics by default



  ## Resource limits & requests
  ##
  resources:
   requests:
     memory: 400Mi

  ## Prometheus StorageSpec for persistent data
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
  ##
  storageSpec: {}
  ## Using PersistentVolumeClaim
  ##
  #  volumeClaimTemplate:
  #    spec:
  #      storageClassName: gluster
  #      accessModes: ["ReadWriteOnce"]
  #      resources:
  #        requests:
  #          storage: 50Gi
  #    selector: {}
